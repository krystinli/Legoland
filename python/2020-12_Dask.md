# Dask 
- parallel computing library
- [tut](https://medium.com/@gongster/dask-an-introduction-and-tutorial-b42f901bcff5)

### Intro
- NumPy, Pandas, and scikit-learn are designed to run on a single core. All the data we run with our code will be **temporarily loaded onto the RAM** of our local system.
- Dask distributes the data across multiple cores of the machine and providing ways to scale Pandas, Scikit-Learn, and Numpy workflows natively, with minimal rewriting.
- PySpark can do a similar job, but it is Daskâ€™s ease of integration into the Python code that makes it so great. 

Dask networks are composed of three pieces:
1. A `centralized scheduler`, which manages workers and assigns the tasks that need to be completed by them.
2. Many `workers`, which do the computation, hold onto results, and communicate results to each other.
3. One or multiple `clients`, from which users interact from Jupyter notebooks or scripts and submit work to the scheduler for execution on the workers.
